{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - Ordinary Least Squares (OLS) Method \n",
    "\n",
    "We will perform linear regression using\n",
    "- Scikit Learn's OLS model\n",
    "- Manually coded OLS method\n",
    "\n",
    "\n",
    "The sklearn OLS implementation code is given in this notebook. You will have to implement the OLS method manually on the given dataset (OLS_Data.csv).\n",
    "\n",
    "\n",
    "### OLS\n",
    "\n",
    "OLS is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being predicted) in the given dataset and those predicted by the linear function.\n",
    "\n",
    "OLS finds the optimal parameters by computing a closed-form solution for the **Normal equation**.\n",
    "\n",
    "URL: https://scikit-learn.org/stable/modules/linear_model.html#linear-model\n",
    "\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We will use a dataset (OLS_Data.csv) containing 14 variables (14 dimensional feature)\n",
    "\n",
    "Input variables:\n",
    "X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14\n",
    "\n",
    "Output variable: \n",
    "y\n",
    "\n",
    "### Note:\n",
    "This dataset might have colinearity in the input variables resulting into the singularity problem. It might cause the OLS method not working. You may need to fix the singularity problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: OLS Linear Regression Using Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import det\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from numpy.linalg import matrix_rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First load the data and explore the feature names, target names, etc.\n",
    "\n",
    "Download the \"OLS_Data.csv\" file to load data from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the csv file as a Pandas DataFrame object denoted as \"df\"\n",
    "\n",
    "df = pd.read_csv('OLS_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Check of the Data\n",
    "\n",
    "Let’s take a look at the top five rows using the DataFrame’s head() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>X11</th>\n",
       "      <th>X12</th>\n",
       "      <th>X13</th>\n",
       "      <th>X14</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>0.02731</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>0.02729</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>0.03237</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>0.06905</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        X1    X2    X3  X4     X5     X6    X7      X8  X9  X10   X11     X12  \\\n",
       "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296  15.3  396.90   \n",
       "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242  17.8  396.90   \n",
       "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242  17.8  392.83   \n",
       "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222  18.7  394.63   \n",
       "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222  18.7  396.90   \n",
       "\n",
       "    X13      X14     y  \n",
       "0  4.98  0.00632  24.0  \n",
       "1  9.14  0.02731  21.6  \n",
       "2  4.03  0.02729  34.7  \n",
       "3  2.94  0.03237  33.4  \n",
       "4  5.33  0.06905  36.2  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Data\n",
    "\n",
    "DataFrame’s info() method is useful to get a quick description of the data, in particular the total number of rows, and each attribute’s type and number of non-null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 15 columns):\n",
      "X1     506 non-null float64\n",
      "X2     506 non-null float64\n",
      "X3     506 non-null float64\n",
      "X4     506 non-null int64\n",
      "X5     506 non-null float64\n",
      "X6     506 non-null float64\n",
      "X7     506 non-null float64\n",
      "X8     506 non-null float64\n",
      "X9     506 non-null int64\n",
      "X10    506 non-null int64\n",
      "X11    506 non-null float64\n",
      "X12    506 non-null float64\n",
      "X13    506 non-null float64\n",
      "X14    506 non-null float64\n",
      "y      506 non-null float64\n",
      "dtypes: float64(12), int64(3)\n",
      "memory usage: 59.4 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Matrix: Feature Correlations\n",
    "\n",
    "Check if the data matrix has colinearity (1 or close to 1) in its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X (N x d):\n",
      "\n",
      "           X1    X2     X3  X4     X5     X6     X7      X8  X9  X10   X11  \\\n",
      "0     0.00632  18.0   2.31   0  0.538  6.575   65.2  4.0900   1  296  15.3   \n",
      "1     0.02731   0.0   7.07   0  0.469  6.421   78.9  4.9671   2  242  17.8   \n",
      "2     0.02729   0.0   7.07   0  0.469  7.185   61.1  4.9671   2  242  17.8   \n",
      "3     0.03237   0.0   2.18   0  0.458  6.998   45.8  6.0622   3  222  18.7   \n",
      "4     0.06905   0.0   2.18   0  0.458  7.147   54.2  6.0622   3  222  18.7   \n",
      "5     0.02985   0.0   2.18   0  0.458  6.430   58.7  6.0622   3  222  18.7   \n",
      "6     0.08829  12.5   7.87   0  0.524  6.012   66.6  5.5605   5  311  15.2   \n",
      "7     0.14455  12.5   7.87   0  0.524  6.172   96.1  5.9505   5  311  15.2   \n",
      "8     0.21124  12.5   7.87   0  0.524  5.631  100.0  6.0821   5  311  15.2   \n",
      "9     0.17004  12.5   7.87   0  0.524  6.004   85.9  6.5921   5  311  15.2   \n",
      "10    0.22489  12.5   7.87   0  0.524  6.377   94.3  6.3467   5  311  15.2   \n",
      "11    0.11747  12.5   7.87   0  0.524  6.009   82.9  6.2267   5  311  15.2   \n",
      "12    0.09378  12.5   7.87   0  0.524  5.889   39.0  5.4509   5  311  15.2   \n",
      "13    0.62976   0.0   8.14   0  0.538  5.949   61.8  4.7075   4  307  21.0   \n",
      "14    0.63796   0.0   8.14   0  0.538  6.096   84.5  4.4619   4  307  21.0   \n",
      "15    0.62739   0.0   8.14   0  0.538  5.834   56.5  4.4986   4  307  21.0   \n",
      "16    1.05393   0.0   8.14   0  0.538  5.935   29.3  4.4986   4  307  21.0   \n",
      "17    0.78420   0.0   8.14   0  0.538  5.990   81.7  4.2579   4  307  21.0   \n",
      "18    0.80271   0.0   8.14   0  0.538  5.456   36.6  3.7965   4  307  21.0   \n",
      "19    0.72580   0.0   8.14   0  0.538  5.727   69.5  3.7965   4  307  21.0   \n",
      "20    1.25179   0.0   8.14   0  0.538  5.570   98.1  3.7979   4  307  21.0   \n",
      "21    0.85204   0.0   8.14   0  0.538  5.965   89.2  4.0123   4  307  21.0   \n",
      "22    1.23247   0.0   8.14   0  0.538  6.142   91.7  3.9769   4  307  21.0   \n",
      "23    0.98843   0.0   8.14   0  0.538  5.813  100.0  4.0952   4  307  21.0   \n",
      "24    0.75026   0.0   8.14   0  0.538  5.924   94.1  4.3996   4  307  21.0   \n",
      "25    0.84054   0.0   8.14   0  0.538  5.599   85.7  4.4546   4  307  21.0   \n",
      "26    0.67191   0.0   8.14   0  0.538  5.813   90.3  4.6820   4  307  21.0   \n",
      "27    0.95577   0.0   8.14   0  0.538  6.047   88.8  4.4534   4  307  21.0   \n",
      "28    0.77299   0.0   8.14   0  0.538  6.495   94.4  4.4547   4  307  21.0   \n",
      "29    1.00245   0.0   8.14   0  0.538  6.674   87.3  4.2390   4  307  21.0   \n",
      "..        ...   ...    ...  ..    ...    ...    ...     ...  ..  ...   ...   \n",
      "476   4.87141   0.0  18.10   0  0.614  6.484   93.6  2.3053  24  666  20.2   \n",
      "477  15.02340   0.0  18.10   0  0.614  5.304   97.3  2.1007  24  666  20.2   \n",
      "478  10.23300   0.0  18.10   0  0.614  6.185   96.7  2.1705  24  666  20.2   \n",
      "479  14.33370   0.0  18.10   0  0.614  6.229   88.0  1.9512  24  666  20.2   \n",
      "480   5.82401   0.0  18.10   0  0.532  6.242   64.7  3.4242  24  666  20.2   \n",
      "481   5.70818   0.0  18.10   0  0.532  6.750   74.9  3.3317  24  666  20.2   \n",
      "482   5.73116   0.0  18.10   0  0.532  7.061   77.0  3.4106  24  666  20.2   \n",
      "483   2.81838   0.0  18.10   0  0.532  5.762   40.3  4.0983  24  666  20.2   \n",
      "484   2.37857   0.0  18.10   0  0.583  5.871   41.9  3.7240  24  666  20.2   \n",
      "485   3.67367   0.0  18.10   0  0.583  6.312   51.9  3.9917  24  666  20.2   \n",
      "486   5.69175   0.0  18.10   0  0.583  6.114   79.8  3.5459  24  666  20.2   \n",
      "487   4.83567   0.0  18.10   0  0.583  5.905   53.2  3.1523  24  666  20.2   \n",
      "488   0.15086   0.0  27.74   0  0.609  5.454   92.7  1.8209   4  711  20.1   \n",
      "489   0.18337   0.0  27.74   0  0.609  5.414   98.3  1.7554   4  711  20.1   \n",
      "490   0.20746   0.0  27.74   0  0.609  5.093   98.0  1.8226   4  711  20.1   \n",
      "491   0.10574   0.0  27.74   0  0.609  5.983   98.8  1.8681   4  711  20.1   \n",
      "492   0.11132   0.0  27.74   0  0.609  5.983   83.5  2.1099   4  711  20.1   \n",
      "493   0.17331   0.0   9.69   0  0.585  5.707   54.0  2.3817   6  391  19.2   \n",
      "494   0.27957   0.0   9.69   0  0.585  5.926   42.6  2.3817   6  391  19.2   \n",
      "495   0.17899   0.0   9.69   0  0.585  5.670   28.8  2.7986   6  391  19.2   \n",
      "496   0.28960   0.0   9.69   0  0.585  5.390   72.9  2.7986   6  391  19.2   \n",
      "497   0.26838   0.0   9.69   0  0.585  5.794   70.6  2.8927   6  391  19.2   \n",
      "498   0.23912   0.0   9.69   0  0.585  6.019   65.3  2.4091   6  391  19.2   \n",
      "499   0.17783   0.0   9.69   0  0.585  5.569   73.5  2.3999   6  391  19.2   \n",
      "500   0.22438   0.0   9.69   0  0.585  6.027   79.7  2.4982   6  391  19.2   \n",
      "501   0.06263   0.0  11.93   0  0.573  6.593   69.1  2.4786   1  273  21.0   \n",
      "502   0.04527   0.0  11.93   0  0.573  6.120   76.7  2.2875   1  273  21.0   \n",
      "503   0.06076   0.0  11.93   0  0.573  6.976   91.0  2.1675   1  273  21.0   \n",
      "504   0.10959   0.0  11.93   0  0.573  6.794   89.3  2.3889   1  273  21.0   \n",
      "505   0.04741   0.0  11.93   0  0.573  6.030   80.8  2.5050   1  273  21.0   \n",
      "\n",
      "        X12    X13       X14     y  \n",
      "0    396.90   4.98   0.00632  24.0  \n",
      "1    396.90   9.14   0.02731  21.6  \n",
      "2    392.83   4.03   0.02729  34.7  \n",
      "3    394.63   2.94   0.03237  33.4  \n",
      "4    396.90   5.33   0.06905  36.2  \n",
      "5    394.12   5.21   0.02985  28.7  \n",
      "6    395.60  12.43   0.08829  22.9  \n",
      "7    396.90  19.15   0.14455  27.1  \n",
      "8    386.63  29.93   0.21124  16.5  \n",
      "9    386.71  17.10   0.17004  18.9  \n",
      "10   392.52  20.45   0.22489  15.0  \n",
      "11   396.90  13.27   0.11747  18.9  \n",
      "12   390.50  15.71   0.09378  21.7  \n",
      "13   396.90   8.26   0.62976  20.4  \n",
      "14   380.02  10.26   0.63796  18.2  \n",
      "15   395.62   8.47   0.62739  19.9  \n",
      "16   386.85   6.58   1.05393  23.1  \n",
      "17   386.75  14.67   0.78420  17.5  \n",
      "18   288.99  11.69   0.80271  20.2  \n",
      "19   390.95  11.28   0.72580  18.2  \n",
      "20   376.57  21.02   1.25179  13.6  \n",
      "21   392.53  13.83   0.85204  19.6  \n",
      "22   396.90  18.72   1.23247  15.2  \n",
      "23   394.54  19.88   0.98843  14.5  \n",
      "24   394.33  16.30   0.75026  15.6  \n",
      "25   303.42  16.51   0.84054  13.9  \n",
      "26   376.88  14.81   0.67191  16.6  \n",
      "27   306.38  17.28   0.95577  14.8  \n",
      "28   387.94  12.80   0.77299  18.4  \n",
      "29   380.23  11.98   1.00245  21.0  \n",
      "..      ...    ...       ...   ...  \n",
      "476  396.21  18.68   4.87141  16.7  \n",
      "477  349.48  24.91  15.02340  12.0  \n",
      "478  379.70  18.03  10.23300  14.6  \n",
      "479  383.32  13.11  14.33370  21.4  \n",
      "480  396.90  10.74   5.82401  23.0  \n",
      "481  393.07   7.74   5.70818  23.7  \n",
      "482  395.28   7.01   5.73116  25.0  \n",
      "483  392.92  10.42   2.81838  21.8  \n",
      "484  370.73  13.34   2.37857  20.6  \n",
      "485  388.62  10.58   3.67367  21.2  \n",
      "486  392.68  14.98   5.69175  19.1  \n",
      "487  388.22  11.45   4.83567  20.6  \n",
      "488  395.09  18.06   0.15086  15.2  \n",
      "489  344.05  23.97   0.18337   7.0  \n",
      "490  318.43  29.68   0.20746   8.1  \n",
      "491  390.11  18.07   0.10574  13.6  \n",
      "492  396.90  13.35   0.11132  20.1  \n",
      "493  396.90  12.01   0.17331  21.8  \n",
      "494  396.90  13.59   0.27957  24.5  \n",
      "495  393.29  17.60   0.17899  23.1  \n",
      "496  396.90  21.14   0.28960  19.7  \n",
      "497  396.90  14.10   0.26838  18.3  \n",
      "498  396.90  12.92   0.23912  21.2  \n",
      "499  395.77  15.10   0.17783  17.5  \n",
      "500  396.90  14.33   0.22438  16.8  \n",
      "501  391.99   9.67   0.06263  22.4  \n",
      "502  396.90   9.08   0.04527  20.6  \n",
      "503  396.90   5.64   0.06076  23.9  \n",
      "504  393.45   6.48   0.10959  22.0  \n",
      "505  396.90   7.88   0.04741  11.9  \n",
      "\n",
      "[506 rows x 15 columns]\n",
      "\n",
      "Rank of X:\n",
      "14\n",
      "\n",
      "Z = X^T.X (d x d):\n",
      "\n",
      "               X1            X2            X3           X4             X5  \\\n",
      "X1   4.397034e+04  4.687027e+02  3.247910e+04     64.80846    1226.123170   \n",
      "X2   4.687027e+02  3.400290e+05  2.090309e+04    270.00000    2484.441750   \n",
      "X3   3.247910e+04  2.090309e+04  8.652563e+04    445.17000    3432.395360   \n",
      "X4   6.480846e+01  2.700000e+02  4.451700e+02     35.00000      20.769900   \n",
      "X5   1.226123e+03  2.484442e+03  3.432395e+03     20.76990     162.470380   \n",
      "X6   1.082195e+04  3.871847e+04  3.446182e+04    228.18600    1751.519414   \n",
      "X7   1.685150e+05  2.054854e+05  4.493135e+05   2712.50000   20452.201630   \n",
      "X8   3.466275e+03  3.829929e+04  1.622067e+04    106.03980     970.389866   \n",
      "X9   4.111867e+04  2.291800e+04  7.176565e+04    326.00000    2991.835900   \n",
      "X10  1.173073e+06  1.722954e+06  2.721349e+06  13519.00000  121170.623200   \n",
      "X11  3.647155e+04  9.613215e+04  1.068753e+05    612.20000    5203.955460   \n",
      "X12  4.994553e+05  2.239605e+06  1.897025e+06  13054.91000   98079.345829   \n",
      "X13  3.726842e+04  3.801968e+04  8.624070e+04    393.46000    3798.325197   \n",
      "X14  4.397034e+04  4.687027e+02  3.247910e+04     64.80846    1226.123170   \n",
      "y    2.568710e+04  1.686078e+05  1.115641e+05    995.40000    6094.427910   \n",
      "\n",
      "               X6            X7             X8            X9           X10  \\\n",
      "X1   1.082195e+04  1.685150e+05    3466.274558  4.111867e+04  1.173073e+06   \n",
      "X2   3.871847e+04  2.054854e+05   38299.294000  2.291800e+04  1.722954e+06   \n",
      "X3   3.446182e+04  4.493135e+05   16220.673289  7.176565e+04  2.721349e+06   \n",
      "X4   2.281860e+02  2.712500e+03     106.039800  3.260000e+02  1.351900e+04   \n",
      "X5   1.751519e+03  2.045220e+04     970.389866  2.991836e+03  1.211706e+05   \n",
      "X6   2.023460e+04  2.156702e+05   12221.680651  2.971903e+04  1.280740e+06   \n",
      "X7   2.156702e+05  2.779615e+06  109297.470090  3.877982e+05  1.537874e+07   \n",
      "X8   1.222168e+04  1.092975e+05    9526.766239  1.375818e+04  6.881538e+05   \n",
      "X9   2.971903e+04  3.877982e+05   13758.179000  8.443000e+04  2.647159e+06   \n",
      "X10  1.280740e+06  1.537874e+07  688153.759100  2.647159e+06  9.867314e+07   \n",
      "X11  5.841597e+04  6.484349e+05   34904.819990  9.360130e+04  3.897240e+06   \n",
      "X12  1.138381e+06  1.202121e+07  713218.526517  1.545044e+06  7.024450e+07   \n",
      "X13  3.868179e+04  5.001916e+05   20523.539880  7.648425e+04  2.944349e+06   \n",
      "X14  1.082195e+04  1.685150e+05    3466.274558  4.111867e+04  1.173073e+06   \n",
      "y    7.392408e+04  7.325811e+05   45713.874170  9.344510e+04  4.287798e+06   \n",
      "\n",
      "              X11           X12           X13           X14             y  \n",
      "X1   3.647155e+04  4.994553e+05  3.726842e+04  4.397034e+04  2.568710e+04  \n",
      "X2   9.613215e+04  2.239605e+06  3.801968e+04  4.687027e+02  1.686078e+05  \n",
      "X3   1.068753e+05  1.897025e+06  8.624070e+04  3.247910e+04  1.115641e+05  \n",
      "X4   6.122000e+02  1.305491e+04  3.934600e+02  6.480846e+01  9.954000e+02  \n",
      "X5   5.203955e+03  9.807935e+04  3.798325e+03  1.226123e+03  6.094428e+03  \n",
      "X6   5.841597e+04  1.138381e+06  3.868179e+04  1.082195e+04  7.392408e+04  \n",
      "X7   6.484349e+05  1.202121e+07  5.001916e+05  1.685150e+05  7.325811e+05  \n",
      "X8   3.490482e+04  7.132185e+05  2.052354e+04  3.466275e+03  4.571387e+04  \n",
      "X9   9.360130e+04  1.545044e+06  7.648425e+04  4.111867e+04  9.344510e+04  \n",
      "X10  3.897240e+06  7.024450e+07  2.944349e+06  1.173073e+06  4.287798e+06  \n",
      "X11  1.747139e+05  3.313095e+06  1.210809e+05  3.647155e+04  2.053167e+05  \n",
      "X12  3.313095e+06  6.858053e+07  2.163061e+06  4.994553e+05  4.208050e+06  \n",
      "X13  1.210809e+05  2.163061e+06  1.067630e+05  3.726842e+04  1.197992e+05  \n",
      "X14  3.647155e+04  4.994553e+05  3.726842e+04  4.397034e+04  2.568710e+04  \n",
      "y    2.053167e+05  4.208050e+06  1.197992e+05  2.568710e+04  2.996263e+05  \n",
      "\n",
      "Rank of Z:\n",
      "14\n",
      "\n",
      "Determinant of Z (X^T.X):  0.0\n",
      "\n",
      "Eigenvalues:\n",
      "\n",
      "[1.58626433e+08 1.19138206e+07 4.18385538e+05 1.65327589e+05\n",
      " 5.26316603e+04 3.72527368e+04 1.10752091e+04 8.27936848e+03\n",
      " 5.58050173e+03 3.67579742e+03 6.04960626e+02 1.69099772e+02\n",
      " 2.98728122e+01 1.20044821e-10 2.18924313e+00]\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-127065b931d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Invert X^T.X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mZ_inv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nInverse of X^T.X (d x d):\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ_inv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\megha\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->D'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->d'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m     \u001b[0mainv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\megha\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Singular matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nX (N x d):\\n\")\n",
    "print(df)\n",
    "\n",
    "\n",
    "print(\"\\nRank of X:\")\n",
    "print(matrix_rank(df))\n",
    "\n",
    "\n",
    "# Compute X^T.X\n",
    "Z = df.T.dot(df)\n",
    "print(\"\\nZ = X^T.X (d x d):\\n\")\n",
    "print(Z)\n",
    "print(\"\\nRank of Z:\")\n",
    "print(matrix_rank(Z))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nDeterminant of Z (X^T.X): \", det(Z))\n",
    "\n",
    "# Find the eigenvalues of X^T.X\n",
    "# For this particular example (X), not all eigenvalues will be positive\n",
    "# Hence, X^T.X is not positive definite & is NOT invertible\n",
    "\n",
    "eigenValues, eigenVectors = np.linalg.eig(Z)\n",
    "\n",
    "print(\"\\nEigenvalues:\\n\")\n",
    "print(eigenValues)\n",
    "\n",
    "\n",
    "# Invert X^T.X\n",
    "Z_inv = np.linalg.inv(Z)\n",
    "print(\"\\nInverse of X^T.X (d x d):\\n\")\n",
    "print(Z_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Separate Feature Set (Data Matrix X) and Target (1D Vector y)\n",
    "\n",
    "Create a data matrix (X) that contains all features and a 1D target vector (y) containing the target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n",
      "(506,)\n",
      "        X1    X2    X3  X4     X5     X6    X7      X8  X9  X10   X11     X12  \\\n",
      "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296  15.3  396.90   \n",
      "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242  17.8  396.90   \n",
      "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242  17.8  392.83   \n",
      "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222  18.7  394.63   \n",
      "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222  18.7  396.90   \n",
      "\n",
      "    X13      X14  \n",
      "0  4.98  0.00632  \n",
      "1  9.14  0.02731  \n",
      "2  4.03  0.02729  \n",
      "3  2.94  0.03237  \n",
      "4  5.33  0.06905  \n",
      "0    24.0\n",
      "1    21.6\n",
      "2    34.7\n",
      "3    33.4\n",
      "4    36.2\n",
      "Name: y, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# data matrix X\n",
    "X = df.iloc[:,0:14]\n",
    "\n",
    "# target vector y\n",
    "y = df1 = df['y']\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "print(X.head())\n",
    "\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale The Features\n",
    "\n",
    "We should ensure that all features have a similar scale. Otherwise optimization algorithms (e.g., Gradient Descent based algorithms) will take much longer time to converge.\n",
    "\n",
    "Also, regularization techniques are sensitive to the scale of data. Thus, we must scale the features before applying regularization.\n",
    "\n",
    "Use sklearns StandardScaler()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41978194  0.28482986 -1.2879095  ...  0.44105193 -1.0755623\n",
      "  -0.41978194]\n",
      " [-0.41733926 -0.48772236 -0.59338101 ...  0.44105193 -0.49243937\n",
      "  -0.41733926]\n",
      " [-0.41734159 -0.48772236 -0.59338101 ...  0.39642699 -1.2087274\n",
      "  -0.41734159]\n",
      " ...\n",
      " [-0.41344658 -0.48772236  0.11573841 ...  0.44105193 -0.98304761\n",
      "  -0.41344658]\n",
      " [-0.40776407 -0.48772236  0.11573841 ...  0.4032249  -0.86530163\n",
      "  -0.40776407]\n",
      " [-0.41500016 -0.48772236  0.11573841 ...  0.44105193 -0.66905833\n",
      "  -0.41500016]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\megha\\lib\\site-packages\\sklearn\\preprocessing\\data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\users\\megha\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test Dataset\n",
    "\n",
    "Create train and test data (80% & 20%) by usinf sklearn's train_test_split function\n",
    "\n",
    "It should return the following 4 matrices.\n",
    "X_train\n",
    "y_train\n",
    "X_test\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train :  [[ 1.32780421 -0.48772236  1.01599907 ... -0.07887794  1.7181012\n",
      "   1.32780421]\n",
      " [-0.34750602 -0.48772236 -0.43725801 ...  0.42701755 -0.5863558\n",
      "  -0.34750602]\n",
      " [-0.41648392  1.01446252 -0.74074945 ...  0.06113692 -0.67606702\n",
      "  -0.41648392]\n",
      " ...\n",
      " [-0.41877066  2.94584308 -1.3316823  ...  0.37570436 -0.93398678\n",
      "  -0.41877066]\n",
      " [ 0.87825441 -0.48772236  1.01599907 ... -2.70626713  1.48821619\n",
      "   0.87825441]\n",
      " [-0.39389588 -0.48772236 -0.37597609 ... -3.13442533 -0.28358043\n",
      "  -0.39389588]]\n",
      "X_test :  [[-0.40983668 -0.48772236 -1.03402724 ...  0.42570183 -0.50645674\n",
      "  -0.40983668]\n",
      " [-0.41394931  1.22906036 -0.68968118 ...  0.44105193 -1.27881429\n",
      "  -0.41394931]\n",
      " [-0.40821211 -0.48772236  2.42256516 ...  0.36660394  0.75931252\n",
      "  -0.40821211]\n",
      " ...\n",
      " [ 1.21460796 -0.48772236  1.01599907 ... -3.52640114  1.20085994\n",
      "   1.21460796]\n",
      " [-0.41447997 -0.48772236 -0.96982713 ...  0.43107437  0.02900711\n",
      "  -0.41447997]\n",
      " [-0.409448   -0.48772236  0.24705682 ...  0.29116915 -0.52047412\n",
      "  -0.409448  ]]\n",
      "y_train :  477    12.0\n",
      "15     19.9\n",
      "332    19.4\n",
      "423    13.4\n",
      "19     18.2\n",
      "325    24.6\n",
      "335    21.1\n",
      "56     24.7\n",
      "437     8.7\n",
      "409    27.5\n",
      "334    20.7\n",
      "181    36.2\n",
      "227    31.6\n",
      "434    11.7\n",
      "180    39.8\n",
      "25     13.9\n",
      "493    21.8\n",
      "238    23.7\n",
      "244    17.6\n",
      "250    24.4\n",
      "418     8.8\n",
      "117    19.2\n",
      "42     25.3\n",
      "322    20.4\n",
      "347    23.1\n",
      "182    37.9\n",
      "155    15.6\n",
      "280    45.4\n",
      "126    15.7\n",
      "329    22.6\n",
      "       ... \n",
      "276    33.2\n",
      "443    15.4\n",
      "191    30.5\n",
      "385     7.2\n",
      "293    23.9\n",
      "413    16.3\n",
      "343    23.9\n",
      "257    50.0\n",
      "308    22.8\n",
      "149    15.4\n",
      "130    19.2\n",
      "151    19.6\n",
      "359    22.6\n",
      "99     33.2\n",
      "372    50.0\n",
      "87     22.2\n",
      "458    14.9\n",
      "330    19.8\n",
      "214    23.7\n",
      "466    19.0\n",
      "121    20.3\n",
      "505    11.9\n",
      "20     13.6\n",
      "188    29.8\n",
      "71     21.7\n",
      "106    19.5\n",
      "270    21.1\n",
      "348    24.5\n",
      "435    13.4\n",
      "102    18.6\n",
      "Name: y, Length: 404, dtype: float64\n",
      "y_test :  173    23.6\n",
      "274    32.4\n",
      "491    13.6\n",
      "72     22.8\n",
      "452    16.1\n",
      "76     20.0\n",
      "316    17.8\n",
      "140    14.0\n",
      "471    19.6\n",
      "500    16.8\n",
      "218    21.5\n",
      "9      18.9\n",
      "414     7.0\n",
      "78     21.2\n",
      "323    18.5\n",
      "473    29.8\n",
      "124    18.8\n",
      "388    10.2\n",
      "195    50.0\n",
      "448    14.1\n",
      "271    25.2\n",
      "278    29.1\n",
      "30     12.7\n",
      "501    22.4\n",
      "421    14.2\n",
      "474    13.8\n",
      "79     20.3\n",
      "454    14.9\n",
      "210    21.7\n",
      "497    18.3\n",
      "       ... \n",
      "444    10.8\n",
      "355    20.6\n",
      "77     20.8\n",
      "398     5.0\n",
      "104    20.1\n",
      "203    48.5\n",
      "381    10.9\n",
      "489     7.0\n",
      "69     20.9\n",
      "408    17.2\n",
      "255    20.9\n",
      "392     9.7\n",
      "312    19.4\n",
      "234    29.0\n",
      "460    16.4\n",
      "324    25.0\n",
      "93     25.0\n",
      "137    17.1\n",
      "176    23.2\n",
      "417    10.4\n",
      "131    19.6\n",
      "346    17.2\n",
      "365    27.5\n",
      "132    23.0\n",
      "371    50.0\n",
      "412    17.9\n",
      "436     9.6\n",
      "411    17.2\n",
      "86     22.5\n",
      "75     21.4\n",
      "Name: y, Length: 102, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"X_train : \", X_train)\n",
    "print(\"X_test : \", X_test)\n",
    "print(\"y_train : \", y_train)\n",
    "print(\"y_test : \", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Models\n",
    "\n",
    "We will use the following linear regression models.\n",
    "\n",
    "- Ordinary least squares (OLS) Linear Regression (by solving the Normal Equation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We will use two evaluation metrics.\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "- Coefficient of Determination ($R^2$ or $r^2$)\n",
    "\n",
    "\n",
    "### Note on $R^2$:\n",
    "R-squared is a statistical measure of how close the data are to the fitted regression line. \n",
    "\n",
    "R-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variable(s).\n",
    "\n",
    "$R^2 = \\frac{Explained Variation}{Total Variation}$\n",
    "\n",
    "R-squared is always between 0 and 100%:\n",
    "\n",
    "- 0% indicates that the model explains none of the variability of the response data around its mean.\n",
    "- 100% indicates that the model explains all the variability of the response data around its mean.\n",
    "\n",
    "\n",
    "#### <font color=red>In general, the higher the R-squared, the better the model fits your data.</font>\n",
    "\n",
    "\n",
    "#### Compute $R^2$ using the sklearn:\n",
    "\n",
    "- The \"r2_score\" function from sklearn.metrics\n",
    "\n",
    "#### Compute MSE using the sklearn:\n",
    "\n",
    "- The \"mean_squared_error\" function from sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Ordinary Least Squares (OLS) Linear Regression (by solving the Normal Equation)\n",
    "\n",
    "\n",
    "#### Sklearn's OLS model implementation code is given for you to review.\n",
    "\n",
    "Then, you will have to manually code the OLS method.\n",
    "\n",
    "\n",
    "#### <font color=red>The MSE and $r^2$ error values from your manually coded OLS method must match with sklearn LinearRegressor's obtained values.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: \n",
      " 22.4852682393169\n",
      "Coefficients: \n",
      " [-0.48574711  0.70155562  0.27675212  0.70653152 -1.99143043  3.11571836\n",
      " -0.17706021 -3.04577065  2.28278471 -1.79260468 -1.97995351  1.12649864\n",
      " -3.62814937 -0.48574711]\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "\n",
      "Mean squared error: 21.64\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.75\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Create the sklearn OLS linear regression object\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# The intercept\n",
    "print(\"Intercept: \\n\", lin_reg.intercept_)\n",
    "\n",
    "# The coefficients\n",
    "print(\"Coefficients: \\n\", lin_reg.coef_)\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "# Make prediction \n",
    "y_train_predicted = lin_reg.predict(X_train)\n",
    "\n",
    "\n",
    "print(\"\\nMean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted))\n",
    "\n",
    "\n",
    "# To compute \n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted))\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % lin_reg.score(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Sklearn OLS Model Using Test Data \n",
    "\n",
    "We evaluate the trained model on the test data.\n",
    "\n",
    "The goal is to see how the model performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 24.29\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Make prediction \n",
    "y_test_predicted = lin_reg.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_test_predicted))\n",
    "\n",
    "\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Coded OLS Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determinant of (X_train_bias^T.X_train_bias):  2.7661640728253403e+19\n",
      "\n",
      "The weight vector:\n",
      " [22.48311009 -4.0313051   0.70155562  0.27675212  0.70653152 -1.99143043\n",
      "  3.11571836 -0.17706021 -3.04577065  2.28278471 -1.79260468 -1.97995351\n",
      "  1.12649864 -3.62814937  0.86094042]\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error: 26.79\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Manually code the OLS Method for Linear Regression\n",
    "\n",
    "\n",
    "# Add a bias term with the feature vectors to create a new data matrix \"X_train_bias\"\n",
    "X_bias = np.c_[np.ones((X_train.shape[0],1)),X_train]\n",
    "#print(\"X_bias : \", X_bias)\n",
    "\n",
    "\n",
    "# Print the determinant of the dot product of the transpose of X_train_bias and X_train_bias\n",
    "print(\"\\nDeterminant of (X_train_bias^T.X_train_bias): \",det(X_bias.T.dot(X_bias)))\n",
    "\n",
    "\n",
    "# Computes the dot product of the transpose of X_train_bias with itself\n",
    "#  Denote the product as \"z\"\n",
    "z = X_bias.T.dot(X_bias)\n",
    "#print(\"z shape : \", z.shape)\n",
    "\n",
    "#print(\"\\n-------- Fixing the Singularity of (X_bias^T)X_bias ------------\")\n",
    "\n",
    "#The singularity problem can also be fixed by checking the columns which are very closly correlated and dropping one of them.\n",
    "\n",
    "# Create a diagonal matrix that has the dimension of z\n",
    "#diagonal = np.eye(z.shape[0], dtype=float)\n",
    "#print(\"diagonal.shape : \",diagonal.shape)\n",
    "\n",
    "# Add small non-zero numbers on the diagonal\n",
    "#diagonal = diagonal * 0.001\n",
    "#print(\"The diagonal matrix:\\n\", diagonal)\n",
    "\n",
    "\n",
    "# Closed form (OLS) solution for weight vector w \n",
    "w = np.linalg.inv(z).dot(X_bias.T).dot(y_train)\n",
    "\n",
    "print(\"\\nThe weight vector:\\n\",w)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction using the X_train_bias data matrix\n",
    "\n",
    "# The predicted target vector should be named as \"y_train_predicted\"\n",
    "y_predicted_train = X_bias.dot(w)\n",
    "\n",
    "# Compute the MSE\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_predicted_train))\n",
    "\n",
    "\n",
    "# Compute the r^2 score\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]: %.2f\" % r2_score(y_train, y_train_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation on the Performance of the Manually Coded OLS Solution\n",
    "\n",
    "You might get the **Singularity matrix** error.\n",
    "\n",
    "The determinant of the $X_{bias}^T.X_{bias}$ should be 0.\n",
    "\n",
    "There must be colinearity in the columns of the data matrix X.\n",
    "\n",
    "Find which columns are coliner.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying OLS Method on Data Matrix With Colinearity in Columns\n",
    "\n",
    "Solve the singularity problem can by adding small positive numbers on the diagonal of the $X_{bias}^T.X_{bias}$ matrix.\n",
    "\n",
    "This regularization technique is known as **Ridge Regression**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Determinant of (X_train_bias^T.X_train_bias):  2.7661640728253403e+19\n",
      "\n",
      "-------- Fixing the Singularity of (X_bias^T).X_bias ------------\n",
      "\n",
      "The weight vector:\n",
      " [22.48470355 -0.48570052  0.70134767  0.27646427  0.70656892 -1.99107885\n",
      "  3.11580471 -0.17707206 -3.04538474  2.28194626 -1.79183318 -1.97988502\n",
      "  1.12646692 -3.62801954 -0.48570052]\n",
      "\n",
      "----------------------------- Model Evaluation -----------------------------\n",
      "Mean squared error: 21.64\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]:0.75\n"
     ]
    }
   ],
   "source": [
    "# Bayesian (Regularized) OLS Method for Linear Regression: Ridge Regression\n",
    "\n",
    "\n",
    "\n",
    "# Add a bias term with the feature vectors to create a new data matrix \"X_train_bias\"\n",
    "X_bias = np.c_[np.ones((X_train.shape[0],1)),X_train]\n",
    "\n",
    "\n",
    "# Print the determinant of the dot product of the transpose of X_train_bias and X_train_bias\n",
    "print(\"\\nDeterminant of (X_train_bias^T.X_train_bias): \",det(X_bias.T.dot(X_bias)))\n",
    "\n",
    "\n",
    "# Computes the dot product of the transpose of X_train_bias with itself\n",
    "#  Denote the product as \"z\"\n",
    "z = X_bias.T.dot(X_bias)\n",
    "# Create a diagonal matrix that has the dimension of z\n",
    "diagonal = np.eye(z.shape[0], dtype=float)\n",
    "\n",
    "\n",
    "print(\"\\n-------- Fixing the Singularity of (X_bias^T).X_bias ------------\")\n",
    "\n",
    "# Create a diagonal matrix that has the dimension of z; name the matrix as \"diagonal\"\n",
    "\n",
    "#Add small non-zero numbers on the diagonal\n",
    "diagonal = diagonal * 0.01\n",
    "#print(\"The diagonal matrix:\\n\", diagonal)\n",
    "# Add small positive non-zero numbers on the diagonal\n",
    "w = np.linalg.inv(z + diagonal).dot(X_bias.T).dot(y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Closed form (OLS) solution for weight vector w \n",
    "#w = np.linalg.inv(z).dot(X_bias.T).dot(y_train)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nThe weight vector:\\n\",w)\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------- Model Evaluation -----------------------------\")\n",
    "\n",
    "\n",
    "\n",
    "# Make prediction using the X_train_bias data matrix\n",
    "# The predicted target vector should be named as \"y_train_predicted\"\n",
    "y_train_predicted = X_bias.dot(w)\n",
    "\n",
    "# Compute the MSE\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_train, y_train_predicted))\n",
    "\n",
    "\n",
    "# Compute the r^2 score\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]:%.2f\" % r2_score(y_train, y_train_predicted))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model Using Test Data - OLS Linear Regression\n",
    "\n",
    "We evaluate the trained model on the test data.\n",
    "\n",
    "Compute the MSE and $r^2$ score using the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 24.29\n",
      "Coefficient of determination r^2 variance score [1 is perfect prediction]:0.67\n"
     ]
    }
   ],
   "source": [
    "X_bias = np.c_[np.ones((X_test.shape[0],1)),X_test]\n",
    "# print(\"\\nDeterminant of (X_test_bias^T.X_test_bias): \",det(X_bias.T.dot(X_bias)))\n",
    "# z = X_bias.T.dot(X_bias)\n",
    "# print(\"\\n-------- Fixing the Singularity of (X_bias^T).X_bias ------------\")\n",
    "\n",
    "# diagonal = np.eye(z.shape[0], dtype=float)\n",
    "\n",
    "y_test_predicted = X_bias.dot(w)\n",
    "\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(y_test, y_test_predicted))\n",
    "\n",
    "# Compute the r^2 score\n",
    "print(\"Coefficient of determination r^2 variance score [1 is perfect prediction]:%.2f\" % r2_score(y_test, y_test_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Understanding the Singularity Issue and its Solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Why do you think the singularity matrix error occur while using OLS method on the “OLS_Data.csv” dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Its because of the colinearity in the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) To fix the singularity problem of the $X_{bias}^T.X_{bias}$ matrix what non-zero positive number did you add on its diagonal?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Add 100000 on the diagonal of the $X_{bias}^T.X_{bias}$ matrix and report the $MSE$ and the $r^2$ values for the training data set. Explain these results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:Mean squared error: 600.08, Coefficient of determination r^2 variance score [1 is perfect prediction]:-5.91"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) After adding 100000 on the diagonal of the $X_{bias}^T.X_{bias}$ matrix what change did you notice in the weights of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
